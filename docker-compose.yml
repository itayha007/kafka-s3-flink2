# This setup allows you to both connect from within the docker-compose context as well as from services running on your local
# machine but not as part of the docker-compose setup. Any client connecting to the broker from the outside can connect to
# localhost:9092 while services running as part of the docker-compose connect to broker:9093.
#
# To access kafka-ui: http://localhost:7777
#
# I hope this helps someone out there! :)

version: '3'
networks:
  local-kafka:
    driver: bridge

services:
  localstack:
    container_name: "${LOCALSTACK_DOCKER_NAME-localstack-main}"
    image: localstack/localstack:s3-latest
    networks:
      - local-kafka
    ports:
      - "127.0.0.1:4566:4566"
    volumes:
      - "./init-s3.py:/etc/localstack/init/ready.d/init-s3.py"  # ready hook
  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    container_name: zookeeper
    networks:
      - local-kafka
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-kafka:7.0.1
    container_name: broker
    networks:
      - local-kafka
    ports:
      # To learn about configuring Kafka for access across networks see
      # https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/
      - "9092:9092"
      - "9093:9093"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LISTENERS: PLAINTEXT_INTERNAL://0.0.0.0:29092,PLAINTEXT_C://0.0.0.0:9093,PLAINTEXT_L://0.0.0.0:9092,
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT_INTERNAL://broker:29092,PLAINTEXT_L://localhost:9092,PLAINTEXT_C://broker:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT_INTERNAL:PLAINTEXT,PLAINTEXT_L:PLAINTEXT,PLAINTEXT_C:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
  init-all:
    image: python:3.10
    container_name: init-all
    networks:
      - local-kafka
    depends_on:
      - broker
      - localstack
    volumes:
      - ./init-all.py:/app/init-all.py
    working_dir: /app
    environment:
      # tune as you wish:
      BOOTSTRAP_SERVERS: broker:9093
      TOPIC_NAME: input
      BUCKET_NAME: dev-bucket
      S3_ENDPOINT: http://localstack:4566   # from inside docker network
      TOTAL_MESSAGES: "1000"
      LARGE_COUNT: "1"          # set 5â€“10 as you prefer
      LARGE_SIZE_MB: "500"
      SMALL_SIZE_BYTES: "5024"
    command: >
      sh -c "pip install boto3 kafka-python && python init-all.py"
    restart: "no"

  hdfs-namenode:
    image: ghcr.io/apache/hadoop:3.3.6
    command: ["hdfs", "namenode"]
    container_name: hdfs-namenode
    networks:
      - local-kafka
    ports:
      - "8020:8020"
      - "9870:9870"
    environment:
      - CLUSTER_NAME=local-hadoop
      - HDFS_CONF_dfs_permissions_enabled=false
    volumes:
      - hdfs-namenode:/hadoop/dfs/namenode

  hdfs-datanode:
    image: ghcr.io/apache/hadoop:3.3.6
    command: ["hdfs", "datanode"]
    container_name: hdfs-datanode
    networks:
      - local-kafka
    depends_on:
      - hdfs-namenode
    ports:
      - "9864:9864"
      - "9866:9866"
    environment:
      - CLUSTER_NAME=local-hadoop
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:8020
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_datanode_address=0.0.0.0:9866
      - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:9864
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=true
      - HDFS_CONF_dfs_datanode_hostname=localhost
      - HDFS_CONF_dfs_client_use_datanode_hostname=true
    volumes:
      - hdfs-datanode:/hadoop/dfs/datanode

  kafka-ui:
    image: provectuslabs/kafka-ui
    container_name: kafka-ui
    networks:
      - local-kafka
    depends_on:
      - broker
    ports:
      - "7777:8080"
    restart: always
    environment:
      - KAFKA_CLUSTERS_0_NAME=broker
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=broker:9093
      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181

  hive-metastore-postgresql:
    image: postgres:10
    container_name: hive-metastore-postgresql
    networks:
      - local-kafka
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore

  hive-metastore:
    image: apache/hive:3.1.3
    command: ["hive", "--service", "metastore"]
    container_name: hive-metastore
    networks:
      - local-kafka
    depends_on:
      - hive-metastore-postgresql
      - hdfs-namenode
      - hdfs-datanode
    ports:
      - "9083:9083"
    environment:
      HIVE_SITE_CONF_fs_defaultFS: hdfs://hdfs-namenode:8020
      METASTORE_DB_HOST: hive-metastore-postgresql
      METASTORE_DB_PORT: 5432
      METASTORE_DB_USER: hive
      METASTORE_DB_PASSWORD: hive

  hive-server:
    image: apache/hive:3.1.3
    command: ["hive", "--service", "hiveserver2"]
    container_name: hive-server
    networks:
      - local-kafka
    depends_on:
      - hive-metastore
      - hdfs-namenode
      - hdfs-datanode
    ports:
      - "10000:10000"
      - "10002:10002"
    environment:
      HIVE_SITE_CONF_fs_defaultFS: hdfs://hdfs-namenode:8020
      HIVE_SITE_CONF_dfs.replication: 1
      HIVE_METASTORE_URI: thrift://hive-metastore:9083
      SERVICE_PRECONDITION: hive-metastore:9083 hdfs-namenode:8020

  trino:
    image: trinodb/trino:latest
    container_name: trino
    networks:
      - local-kafka
    depends_on:
      - hive-metastore
    ports:
      - "8081:8080"
    volumes:
      - ./trino/catalog:/etc/trino/catalog

volumes:
  hdfs-namenode:
  hdfs-datanode:

